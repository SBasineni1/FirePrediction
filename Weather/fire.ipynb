{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining of the program\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df_tor = pd.read_csv(\\'1950-2021_actual_tornadoes.csv\\')\\ndf_tor.describe(include=\\'all\\').transpose()\\ndf_tor_f = df_tor[[\\'om\\',\\'yr\\',\\'mo\\',\\'dy\\',\\'st\\',\\'mag\\']]\\n#removing duplicate tornado records\\ndf_tor_u =  df_tor_f.drop_duplicates()\\n\\n# using group by to find the maximum F-Scale Tornado happened on that day\\ndf_max = df_tor_u.groupby([\\'yr\\',\\'mo\\',\\'dy\\',\\'st\\'])[\\'mag\\'].max().reset_index(name=\\'max_mag_f_scale\\')\\n# Using group by to add the no of recorded tornadoes happened by date and state\\ndf_cnt = df_tor_u.groupby([\\'yr\\',\\'mo\\',\\'dy\\',\\'st\\']).size().reset_index(name=\\'no_of_tornadoes\\')\\n#Merge the data to prepare a final data frame to refer for the days tornadoes happened.\\ndf_grp = pd.merge(df_max,df_cnt, how=\\'inner\\')\\n#Filtering data by year and state\\ndf_tor_2007 = df_grp[df_grp.yr >= 2007]\\ndf_tor_2007_OK = df_tor_2007[df_tor_2007.st == \"OK\"]\\n\\n#frames = [df_tor_2013_OK, df]\\nframes = pd.merge(df_2007, df_tor_2007_OK, how=\\'left\\',left_on=[\\'Year\\',\\'Month\\',\\'Day\\'] , right_on = [\\'yr\\',\\'mo\\',\\'dy\\'])\\nfinal_frames = frames[[\\'Year\\',\\'Month\\',\\'Day\\',\\'Hour\\',\\'RELTIME\\',\\'PW\\',\\'st\\',\\'max_mag_f_scale\\',\\'no_of_tornadoes\\',\\n                       \\'LCLPRESS\\',\\'LNBHGT\\',\\'LI\\',\\'SI\\',\\'KI\\',\\'TTI\\',\\'CAPE\\',\\'CIN\\']] \\n# Fill the NaN values \\ndf_final =final_frames.fillna(value={\\'st\\':\\'OK\\',\\'no_of_tornadoes\\':0,\\'max_mag_f_scale\\':-9})\\ndf_final.rename(columns = {\\'max_mag_f_scale\\':\\'f_scale\\'}, inplace = True)\\n\\ndf_corr=  df_final[[\\'st\\',\\'f_scale\\',\\'PW\\',\\'LCLPRESS\\',\\'LI\\',\\'SI\\',\\'KI\\',\\'TTI\\',\\'LNBHGT\\',\\'CAPE\\',\\'CIN\\']] \\npearsoncorr = round(df_corr.corr(method=\\'pearson\\'),3)\\nsb.heatmap(pearsoncorr, \\n            xticklabels=pearsoncorr.columns,\\n            yticklabels=pearsoncorr.columns,\\n            cmap=\\'RdBu_r\\',\\n            annot=True,\\n            linewidth=0.5)\\n# df_corr1=  final_frames[final_frames.PW != -99999]\\ndf_corr1=  df_final[[\\'st\\',\\'f_scale\\',\\'no_of_tornadoes\\',\\'PW\\',\\'LCLPRESS\\',\\'LI\\',\\'SI\\',\\'KI\\',\\'TTI\\']] \\n\\n#df_corr2=  final_frames[final_frames.CAPE != -99999]\\ndf_corr2=  df_final[[\\'st\\',\\'f_scale\\',\\'no_of_tornadoes\\',\\'LNBHGT\\',\\'CAPE\\',\\'CIN\\']] \\n\\npearsoncorr2 = df_corr2.corr(method=\\'pearson\\')\\npearsoncorr1 = df_corr1.corr(method=\\'pearson\\')\\npearsoncorr1\\nsb.heatmap(pearsoncorr2, \\n            xticklabels=pearsoncorr2.columns,\\n            yticklabels=pearsoncorr2.columns,\\n            cmap=\\'RdBu_r\\',\\n            annot=True,\\n            linewidth=0.5)\\nsb.heatmap(pearsoncorr1, \\n            xticklabels=pearsoncorr1.columns,\\n            yticklabels=pearsoncorr1.columns,\\n            cmap=\\'RdBu_r\\',\\n            annot=True,\\n            linewidth=0.5)\\n\\n#[frames.Year,frames.Month,frames.Day]\\n#result = pd.concat(frames)\\n \\n\\n#Descriptive Statistics.\\ndf_dsc_Stat= df_corr.describe().transpose()\\n\\n\\n\\n\\n#Getting the Corelation coefficient and the P Value \\ncoff_df = pd.DataFrame(columns=[\\'r\\',\\'p\\'])\\n \\nfor col in df_corr:\\n    print(col)\\n    if pd.api.types.is_numeric_dtype(df_corr[col]):\\n        r,p = stats.pearsonr(df_corr.f_scale,df_corr[col])\\n        coff_df.loc[col] = [round(r,4),round(p,5)]\\n    \\ncoff_df\\n\\ncoff_df.rename(columns = {\\'r\\':\\'corr-coeff\\',\\'p\\':\\'p-value\\'}, inplace = True)\\n\\n\\n# Graphical analysis \\ndf_corr[\\'f_scale\\']=df_corr.f_scale.astype(str)\\nsb.histplot(data=df_corr[df_corr.f_scale != \\'-9.0\\'], x= \\'f_scale\\' )\\n#import seaborn as sns\\nsb.scatterplot(x=\"CAPE\", y=\"CIN\", hue=\"f_scale\" ,data=df_corr);\\nsb.scatterplot(x=\"CAPE\", y=\"CIN\", hue=\"f_scale\" ,data=df_corr[df_corr.f_scale != \\'-9.0\\']);\\nsb.boxplot(x=df_corr[\\'CAPE\\'], y=df_corr[\\'f_scale\\'], showmeans=True)\\nsb.displot(x=\\'CAPE\\', col=\\'f_scale\\', data=df_corr[df_corr.f_scale != \\'-9.0\\'], linewidth=3, kde=True);\\nsb.displot(x=\\'CAPE\\',  data=df_corr[df_corr.f_scale != \\'-9.0\\'], linewidth=1, kde=True);'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 16 18:52:51 2022\n",
    "This program reads the weather data file which has Header and detail record. \n",
    "The header record which is identified as it starts with # . For my research I only need the \n",
    "header data. The following program reads the IGRA data derived file and parses the file to \n",
    "extract header files and writes into a new file to import for further analysis\n",
    "@author: Suchit Basineni\n",
    "\"\"\"\n",
    "# Importing Libraries\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "# import numpy as np\n",
    "#import os\n",
    "\n",
    "# packages for Decision tree model\n",
    "#\n",
    "#*********************************************************************************** \n",
    "# Importing the required packages\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# ********************************************************************\n",
    "#Create a variable for the file name\n",
    "''''filename = \"YNWC1-2021-2023-inputfilev1.0.csv\"\n",
    "filenamew = \"Fire_data.txt\"\n",
    "#initialize an empty list\n",
    "word_list= []\n",
    "#w tells python we are opening the file to write into it\n",
    "outfile = open(filenamew, 'w')\n",
    "#Open the file\n",
    "infile = open(filename, 'r') \n",
    "lines = infile.readlines() \n",
    "for line in lines: #lines is a list with each item representing a line of the file\n",
    "\tif ',' in line:\n",
    "         word_list.append(line)\t\n",
    "outfile.writelines(word_list) \n",
    "outfile.close()\n",
    "infile.close() #close the file when you're done!\n",
    "'''''\n",
    "\n",
    "#cols = ['Station_ID','Date_Time\tair_temp_set_1','relative_humidity_set_1','wind_speed_set_1','wind_direction_set_1',\n",
    "#        'wind_gust_set_1','snow_depth_set_1','solar_radiation_set_1','precip_accum_set_1','peak_wind_speed_set_1',\n",
    "#        'fuel_temp_set_1','fuel_moisture_set_1','volt_set_1','snow_interval_set_1','peak_wind_direction_set_1','wind_chill_set_1d',\n",
    "#        'wind_cardinal_direction_set_1d','heat_index_set_1d','dew_point_temperature_set_1d']\n",
    "\n",
    "# Read the fixed format file into a Pandas Data Frame\n",
    "\n",
    "#balance_data = pd.read_fwf('YNWC1-2021-2023-inputfilev1.0.csv', sep=',',\n",
    "#                 header=1,names = None,widths=None,skip_baln_lines=True)\n",
    "                 #[12,5,3,3,3,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
    "                 #names=cols)\n",
    "#balance_data.shape\n",
    "## Displaying dataset information\n",
    "# print(\"Dataset Length: \", len(balance_data))\n",
    "# print(\"Dataset Shape: \", balance_data.shape)\n",
    "# print(\"Dataset: \", balance_data.head())\n",
    "#print(balance_data)\n",
    "#balance_data.describe(include='all')\n",
    "\n",
    "# Decision tree\n",
    "print('Begining of the program')\n",
    "def importdata():\n",
    "    balance_data = pd.read_csv(\n",
    "        'YNWC1-2021-2023-inputfilev1.0.csv',\n",
    "        sep=',', header=1)\n",
    "    balance_data.shape\n",
    "    print(balance_data)\n",
    "    # Displaying dataset information\n",
    "    print(\"Dataset Length: \", len(balance_data))\n",
    "    print(\"Dataset Shape: \", balance_data.shape)\n",
    "    print(\"Dataset: \", balance_data.head())\n",
    "     \n",
    "    return balance_data\n",
    "\n",
    "# Function to split the dataset into features and target variables\n",
    "def splitdataset(balance_data):\n",
    " \n",
    "    # Separating the target variable\n",
    "    X = balance_data.values[:, 1:5]\n",
    "    Y = balance_data.values[:, 0]\n",
    " \n",
    "    # Splitting the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, Y, test_size=0.3, random_state=100)\n",
    " \n",
    "    return X, Y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_using_gini(X_train, X_test, y_train):\n",
    " \n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                      random_state=100, max_depth=3, min_samples_leaf=5)\n",
    " \n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "\n",
    "  \n",
    "def tarin_using_entropy(X_train, X_test, y_train):\n",
    " \n",
    "    # Decision tree with entropy\n",
    "    clf_entropy = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\", random_state=100,\n",
    "        max_depth=3, min_samples_leaf=5)\n",
    " \n",
    "    # Performing training\n",
    "    clf_entropy.fit(X_train, y_train)\n",
    "    return clf_entropy\n",
    "\n",
    "# Function to make predictions\n",
    "def prediction(X_test, clf_object):\n",
    "    y_pred = clf_object.predict(X_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred\n",
    " \n",
    "# Placeholder function for cal_accuracy\n",
    "def cal_accuracy(y_test, y_pred):\n",
    "    print(\"Confusion Matrix: \",\n",
    "          confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy : \",\n",
    "          accuracy_score(y_test, y_pred)*100)\n",
    "    print(\"Report : \",\n",
    "          classification_report(y_test, y_pred))\n",
    "\n",
    "   \n",
    "# Function to plot the decision tree\n",
    "def plot_decision_tree(clf_object, feature_names, class_names):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(clf_object, filled=True, feature_names=feature_names, class_names=class_names, rounded=True)\n",
    "    plt.show()\n",
    "\n",
    " #if __name__ == \"__main__\":\n",
    "    data = importdata()\n",
    "    X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
    "\n",
    "    clf_gini = train_using_gini(X_train, X_test, y_train)\n",
    "    clf_entropy = tarin_using_entropy(X_train, X_test, y_train)\n",
    " \n",
    "    # Visualizing the Decision Trees\n",
    "    plot_decision_tree(clf_gini, ['X1', 'X2', 'X3', 'X4'], ['L', 'B', 'R'])\n",
    "    plot_decision_tree(clf_entropy, ['X1', 'X2', 'X3', 'X4'], ['L', 'B', 'R'])   \n",
    "\n",
    "\n",
    "#df_2007 = df[df.Year >= 2007 ]\n",
    "\n",
    "#df_2007 = df_2007[(df_2007.PW != -99999)]\n",
    "#df_2007 = df_2007[(df_2007.CAPE != -99999)]  \n",
    "# No of Rows and columns                \n",
    "#df.shape\n",
    "\n",
    "# Variable types \n",
    "#df.dtypes\n",
    "\n",
    "#Used to import the actual tornado reports\n",
    "'''df_tor = pd.read_csv('1950-2021_actual_tornadoes.csv')\n",
    "df_tor.describe(include='all').transpose()\n",
    "df_tor_f = df_tor[['om','yr','mo','dy','st','mag']]\n",
    "#removing duplicate tornado records\n",
    "df_tor_u =  df_tor_f.drop_duplicates()\n",
    "\n",
    "# using group by to find the maximum F-Scale Tornado happened on that day\n",
    "df_max = df_tor_u.groupby(['yr','mo','dy','st'])['mag'].max().reset_index(name='max_mag_f_scale')\n",
    "# Using group by to add the no of recorded tornadoes happened by date and state\n",
    "df_cnt = df_tor_u.groupby(['yr','mo','dy','st']).size().reset_index(name='no_of_tornadoes')\n",
    "#Merge the data to prepare a final data frame to refer for the days tornadoes happened.\n",
    "df_grp = pd.merge(df_max,df_cnt, how='inner')\n",
    "#Filtering data by year and state\n",
    "df_tor_2007 = df_grp[df_grp.yr >= 2007]\n",
    "df_tor_2007_OK = df_tor_2007[df_tor_2007.st == \"OK\"]\n",
    "\n",
    "#frames = [df_tor_2013_OK, df]\n",
    "frames = pd.merge(df_2007, df_tor_2007_OK, how='left',left_on=['Year','Month','Day'] , right_on = ['yr','mo','dy'])\n",
    "final_frames = frames[['Year','Month','Day','Hour','RELTIME','PW','st','max_mag_f_scale','no_of_tornadoes',\n",
    "                       'LCLPRESS','LNBHGT','LI','SI','KI','TTI','CAPE','CIN']] \n",
    "# Fill the NaN values \n",
    "df_final =final_frames.fillna(value={'st':'OK','no_of_tornadoes':0,'max_mag_f_scale':-9})\n",
    "df_final.rename(columns = {'max_mag_f_scale':'f_scale'}, inplace = True)\n",
    "\n",
    "df_corr=  df_final[['st','f_scale','PW','LCLPRESS','LI','SI','KI','TTI','LNBHGT','CAPE','CIN']] \n",
    "pearsoncorr = round(df_corr.corr(method='pearson'),3)\n",
    "sb.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5)\n",
    "# df_corr1=  final_frames[final_frames.PW != -99999]\n",
    "df_corr1=  df_final[['st','f_scale','no_of_tornadoes','PW','LCLPRESS','LI','SI','KI','TTI']] \n",
    "\n",
    "#df_corr2=  final_frames[final_frames.CAPE != -99999]\n",
    "df_corr2=  df_final[['st','f_scale','no_of_tornadoes','LNBHGT','CAPE','CIN']] \n",
    "\n",
    "pearsoncorr2 = df_corr2.corr(method='pearson')\n",
    "pearsoncorr1 = df_corr1.corr(method='pearson')\n",
    "pearsoncorr1\n",
    "sb.heatmap(pearsoncorr2, \n",
    "            xticklabels=pearsoncorr2.columns,\n",
    "            yticklabels=pearsoncorr2.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5)\n",
    "sb.heatmap(pearsoncorr1, \n",
    "            xticklabels=pearsoncorr1.columns,\n",
    "            yticklabels=pearsoncorr1.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.5)\n",
    "\n",
    "#[frames.Year,frames.Month,frames.Day]\n",
    "#result = pd.concat(frames)\n",
    " \n",
    "\n",
    "#Descriptive Statistics.\n",
    "df_dsc_Stat= df_corr.describe().transpose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Getting the Corelation coefficient and the P Value \n",
    "coff_df = pd.DataFrame(columns=['r','p'])\n",
    " \n",
    "for col in df_corr:\n",
    "    print(col)\n",
    "    if pd.api.types.is_numeric_dtype(df_corr[col]):\n",
    "        r,p = stats.pearsonr(df_corr.f_scale,df_corr[col])\n",
    "        coff_df.loc[col] = [round(r,4),round(p,5)]\n",
    "    \n",
    "coff_df\n",
    "\n",
    "coff_df.rename(columns = {'r':'corr-coeff','p':'p-value'}, inplace = True)\n",
    "\n",
    "\n",
    "# Graphical analysis \n",
    "df_corr['f_scale']=df_corr.f_scale.astype(str)\n",
    "sb.histplot(data=df_corr[df_corr.f_scale != '-9.0'], x= 'f_scale' )\n",
    "#import seaborn as sns\n",
    "sb.scatterplot(x=\"CAPE\", y=\"CIN\", hue=\"f_scale\" ,data=df_corr);\n",
    "sb.scatterplot(x=\"CAPE\", y=\"CIN\", hue=\"f_scale\" ,data=df_corr[df_corr.f_scale != '-9.0']);\n",
    "sb.boxplot(x=df_corr['CAPE'], y=df_corr['f_scale'], showmeans=True)\n",
    "sb.displot(x='CAPE', col='f_scale', data=df_corr[df_corr.f_scale != '-9.0'], linewidth=3, kde=True);\n",
    "sb.displot(x='CAPE',  data=df_corr[df_corr.f_scale != '-9.0'], linewidth=1, kde=True);'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
